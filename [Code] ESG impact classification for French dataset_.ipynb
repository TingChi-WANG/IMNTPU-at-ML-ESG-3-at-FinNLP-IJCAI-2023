{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "znCHPTVxzg7l"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openpyxl"
      ],
      "metadata": {
        "id": "foZkG5ZdzlSN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "file_path = '/content/FR.xlsx'\n",
        "df = pd.read_excel(file_path)"
      ],
      "metadata": {
        "id": "2ffi3edSzmk1"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the total number of news content\n",
        "total_news_content = len(df)\n",
        "\n",
        "# Calculate unique news titles\n",
        "\n",
        "\n",
        "# Impact Duration Distribution based on the exact values of 0, 1, and 2\n",
        "impact_duration_distribution = df['impact_duration'].value_counts().sort_index()\n",
        "\n",
        "# Impact Level Distribution\n",
        "impact_level_distribution = df['impact_level'].value_counts()\n",
        "\n",
        "# Output the calculated values\n",
        "print(f\"Total News Content: {total_news_content}\")\n",
        "\n",
        "print(\"Impact Duration Distribution:\")\n",
        "print(impact_duration_distribution)\n",
        "print(\"Impact Level Distribution:\")\n",
        "print(impact_level_distribution)"
      ],
      "metadata": {
        "id": "wiItnE5hzoA7",
        "outputId": "28c5c015-2f55-48e3-b947-79494025ada0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total News Content: 10104\n",
            "Impact Duration Distribution:\n",
            "0    3192\n",
            "1    3348\n",
            "2    3564\n",
            "Name: impact_duration, dtype: int64\n",
            "Impact Level Distribution:\n",
            "low       4773\n",
            "medium    4726\n",
            "high       605\n",
            "Name: impact_level, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install sklearn\n",
        "!pip install scikit-learn"
      ],
      "metadata": {
        "id": "MdOxPq6vzp9Y",
        "outputId": "fc6ba4b3-b67a-4ece-8e4d-8bd6e284364f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.37.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n",
            "Collecting sklearn\n",
            "  Using cached sklearn-0.0.post12.tar.gz (2.6 kB)\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from transformers import BertTokenizer, BertModel, BertForSequenceClassification, TrainingArguments, Trainer, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
        "from torch.utils.data import Dataset\n",
        "import torch\n",
        "\n",
        "# Encode the 'impact_duration' labels to numeric values\n",
        "label_encoder = LabelEncoder()\n",
        "df['impact_duration_encoded'] = label_encoder.fit_transform(df['impact_duration'])\n",
        "# Check for NaN values in 'news_content'\n",
        "if df['news_content'].isnull().any():\n",
        "    # Handle NaN values, e.g., by replacing them with a placeholder string\n",
        "    df['news_content'].fillna('No content', inplace=True)\n",
        "\n",
        "# Split the data into training and validation sets (90% training, 10% validation)\n",
        "train_df, val_df = train_test_split(df, test_size=0.1, random_state=42)\n",
        "\n",
        "# Initialize the tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
        "\n",
        "def tokenize_data(df):\n",
        "    # Ensure all entries are strings and handle missing values if necessary\n",
        "    texts = df['news_content'].astype(str).tolist()  # Convert to string to avoid issues\n",
        "    return tokenizer(texts, truncation=True, padding=True, max_length=512)\n",
        "\n",
        "\n",
        "train_encodings = tokenize_data(train_df)\n",
        "val_encodings = tokenize_data(val_df)\n",
        "\n",
        "train_labels = train_df['impact_duration_encoded'].tolist()\n",
        "val_labels = val_df['impact_duration_encoded'].tolist()\n",
        "\n",
        "# Create a custom dataset for PyTorch\n",
        "class NewsDataset(Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "train_dataset = NewsDataset(train_encodings, train_labels)\n",
        "val_dataset = NewsDataset(val_encodings, val_labels)\n",
        "\n",
        "# Load the RoBERTa model\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-multilingual-cased', num_labels=len(label_encoder.classes_))"
      ],
      "metadata": {
        "id": "bNjeAclszrkY",
        "outputId": "c6c1ed42-fa41-45a0-ec1c-4e6c6513a6b5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install transformers -U\n",
        "!pip install accelerate -U\n",
        "!pip install scikit-learn"
      ],
      "metadata": {
        "id": "zzVN_lXyzryU",
        "outputId": "12e48735-9f80-4b64-e540-9119d431c5be",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.37.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.27.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.1.0+cu121)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.9.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=16,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    evaluation_strategy=\"epoch\",\n",
        ")\n",
        "\n",
        "# Compute metrics function for evaluation\n",
        "def compute_metrics(p):\n",
        "    predictions, labels = p\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='weighted')\n",
        "    acc = accuracy_score(labels, predictions)\n",
        "    return {'accuracy': acc, 'f1': f1, 'precision': precision, 'recall': recall}\n",
        "\n",
        "# Initialize the trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n",
        "\n",
        "# Evaluate the model\n",
        "evaluation_results = trainer.evaluate()\n",
        "\n",
        "print(evaluation_results)\n"
      ],
      "metadata": {
        "id": "SvAwMXGa6_CE",
        "outputId": "7b0c7328-03e9-4022-8b82-5d0d6f0e3bea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3411' max='3411' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [3411/3411 12:34, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.408500</td>\n",
              "      <td>0.524287</td>\n",
              "      <td>0.858556</td>\n",
              "      <td>0.859857</td>\n",
              "      <td>0.877763</td>\n",
              "      <td>0.858556</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.284300</td>\n",
              "      <td>0.288305</td>\n",
              "      <td>0.939664</td>\n",
              "      <td>0.939606</td>\n",
              "      <td>0.939592</td>\n",
              "      <td>0.939664</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.136100</td>\n",
              "      <td>0.226402</td>\n",
              "      <td>0.952522</td>\n",
              "      <td>0.952540</td>\n",
              "      <td>0.952584</td>\n",
              "      <td>0.952522</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Checkpoint destination directory ./results/checkpoint-500 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='64' max='64' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [64/64 00:06]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.22640159726142883, 'eval_accuracy': 0.9525222551928784, 'eval_f1': 0.9525400828331279, 'eval_precision': 0.9525842160589271, 'eval_recall': 0.9525222551928784, 'eval_runtime': 6.8511, 'eval_samples_per_second': 147.568, 'eval_steps_per_second': 9.342, 'epoch': 3.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Generate predictions for the training dataset\n",
        "train_predictions = trainer.predict(train_dataset)\n",
        "\n",
        "# Generate predictions for the validation dataset\n",
        "val_predictions = trainer.predict(val_dataset)\n",
        "\n",
        "# The predictions are in logits, so you need to apply softmax to convert to probabilities and then take the argmax to get the predicted labels\n",
        "train_pred_labels = np.argmax(train_predictions.predictions, axis=1)\n",
        "val_pred_labels = np.argmax(val_predictions.predictions, axis=1)\n",
        "\n",
        "# The true labels are already provided as part of the datasets\n",
        "train_true_labels = train_predictions.label_ids\n",
        "val_true_labels = val_predictions.label_ids\n",
        "\n",
        "# Calculate accuracy for the training dataset\n",
        "train_accuracy = accuracy_score(train_true_labels, train_pred_labels)\n",
        "\n",
        "# Calculate accuracy for the validation dataset\n",
        "val_accuracy = accuracy_score(val_true_labels, val_pred_labels)\n",
        "\n",
        "# To print classification reports, you'll need to import it\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Convert the label encoder's classes to strings for use as target names\n",
        "target_names = [str(label) for label in label_encoder.inverse_transform(range(len(label_encoder.classes_)))]\n",
        "\n",
        "# Then use these target names in your classification report\n",
        "print(\"Training Dataset Metrics:\")\n",
        "print(f\"Accuracy: {train_accuracy}\")\n",
        "print(classification_report(train_true_labels, train_pred_labels, target_names=target_names))\n",
        "\n",
        "print(\"Validation Dataset Metrics:\")\n",
        "print(f\"Accuracy: {val_accuracy}\")\n",
        "print(classification_report(val_true_labels, val_pred_labels, target_names=target_names))\n",
        "\n"
      ],
      "metadata": {
        "id": "AyjYLmCh7DND",
        "outputId": "032605c1-d92a-4bad-b072-542100e79a63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Dataset Metrics:\n",
            "Accuracy: 0.9861431870669746\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.99      0.99      2862\n",
            "           1       0.99      0.98      0.98      3037\n",
            "           2       0.99      0.99      0.99      3194\n",
            "\n",
            "    accuracy                           0.99      9093\n",
            "   macro avg       0.99      0.99      0.99      9093\n",
            "weighted avg       0.99      0.99      0.99      9093\n",
            "\n",
            "Validation Dataset Metrics:\n",
            "Accuracy: 0.9525222551928784\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.95      0.95       330\n",
            "           1       0.94      0.95      0.94       311\n",
            "           2       0.96      0.96      0.96       370\n",
            "\n",
            "    accuracy                           0.95      1011\n",
            "   macro avg       0.95      0.95      0.95      1011\n",
            "weighted avg       0.95      0.95      0.95      1011\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openpyxl"
      ],
      "metadata": {
        "id": "eHNarrBY7dlS",
        "outputId": "ece06826-857d-43e7-d3c5-d766bf3148c3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.10/dist-packages (3.1.2)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl) (1.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the test dataset\n",
        "test_dataset_path = '/content/FR.xlsx'\n",
        "test_df = pd.read_excel(test_dataset_path)\n",
        "\n",
        "# Display the first few rows of the test dataset to understand its structure\n",
        "test_df.head()\n"
      ],
      "metadata": {
        "id": "LGf944fw7dip",
        "outputId": "a0129da7-db15-46b7-e557-5f2d8dc8ab3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  impact_level                                       news_content  \\\n",
              "0         high  Des protestataires ont occupé temporairement d...   \n",
              "1         high  Les protestataires ont désigné le ministère co...   \n",
              "2         high  Sous les directives de l'ancien général Sami R...   \n",
              "3         high  Lors d'opérations apparemment coordonnées, des...   \n",
              "4         high  Le ministère des Affaires étrangères a été pri...   \n",
              "\n",
              "   impact_duration  \n",
              "0                0  \n",
              "1                0  \n",
              "2                0  \n",
              "3                0  \n",
              "4                0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-836e3da4-1622-4295-886f-4a7adc79f7bb\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>impact_level</th>\n",
              "      <th>news_content</th>\n",
              "      <th>impact_duration</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>high</td>\n",
              "      <td>Des protestataires ont occupé temporairement d...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>high</td>\n",
              "      <td>Les protestataires ont désigné le ministère co...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>high</td>\n",
              "      <td>Sous les directives de l'ancien général Sami R...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>high</td>\n",
              "      <td>Lors d'opérations apparemment coordonnées, des...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>high</td>\n",
              "      <td>Le ministère des Affaires étrangères a été pri...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-836e3da4-1622-4295-886f-4a7adc79f7bb')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-836e3da4-1622-4295-886f-4a7adc79f7bb button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-836e3da4-1622-4295-886f-4a7adc79f7bb');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-b22b083a-3a8d-48dd-829d-63c56a9d5694\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-b22b083a-3a8d-48dd-829d-63c56a9d5694')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-b22b083a-3a8d-48dd-829d-63c56a9d5694 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "test_df",
              "summary": "{\n  \"name\": \"test_df\",\n  \"rows\": 10104,\n  \"fields\": [\n    {\n      \"column\": \"impact_level\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"samples\": [\n          \"high\",\n          \"low\",\n          \"medium\"\n        ],\n        \"num_unique_values\": 3,\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"news_content\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"samples\": [\n          \"Symboliquement, chaque m\\u00e9gaoctet inutilis\\u00e9 sur le r\\u00e9seau \\u00e9quivaut \\u00e0 100 unit\\u00e9s virtuelles de \\\"gouttes d'eau\\\".\",\n          \"Toutefois, la tension sociale persiste en raison d'une augmentation de 57 % des b\\u00e9n\\u00e9fices d'Amazon en 2021, atteignant 33 milliards de dollars.\",\n          \"Les pr\\u00e9occupations entourant la confidentialit\\u00e9 des donn\\u00e9es sont importantes, en particulier alors que certains \\u00c9tats conservateurs envisagent d'attribuer les m\\u00eames droits aux f\\u0153tus et aux \\u00eatres humains d\\u00e8s la f\\u00e9condation.\"\n        ],\n        \"num_unique_values\": 10062,\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"impact_duration\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 2,\n        \"samples\": [\n          0,\n          1,\n          2\n        ],\n        \"num_unique_values\": 3,\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer\n",
        "from torch.utils.data import Dataset\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Assuming you have already loaded your model and tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
        "\n",
        "# Tokenize the test dataset\n",
        "def tokenize_data(df):\n",
        "    texts = df['news_content'].astype(str).tolist()  # Ensure all entries are strings\n",
        "    return tokenizer(texts, truncation=True, padding=True, max_length=512)\n",
        "\n",
        "test_encodings = tokenize_data(test_df)\n",
        "\n",
        "# Dummy labels for creating the Dataset object, since labels are not needed for prediction\n",
        "test_labels = [0] * len(test_df)\n",
        "\n",
        "class TestDataset(Dataset):\n",
        "    def __init__(self, encodings):\n",
        "        self.encodings = encodings\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encodings.input_ids)\n",
        "\n",
        "test_dataset = TestDataset(test_encodings)\n",
        "\n",
        "# Predict using the trained model\n",
        "predictions = trainer.predict(test_dataset)\n",
        "\n",
        "# Convert predictions to labels\n",
        "predicted_labels = np.argmax(predictions.predictions, axis=1)\n",
        "original_labels = label_encoder.inverse_transform(predicted_labels)\n",
        "\n",
        "# Add predictions to the test DataFrame\n",
        "test_df['predicted_impact_duration'] = original_labels\n",
        "import os\n",
        "print(os.getcwd())\n",
        "\n",
        "# Save the updated DataFrame to a new JSON file in the current working directory\n",
        "output_filename = 'updated_test_dataset_with_predictions.json'\n",
        "test_df.to_json(output_filename, orient='records', lines=True)\n",
        "\n",
        "print(f\"Updated test dataset saved to {output_filename}\")\n"
      ],
      "metadata": {
        "id": "v9kilblx77Fs",
        "outputId": "d9489c4a-b59f-4471-c2fa-8127b2e9d60b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Updated test dataset saved to updated_test_dataset_with_predictions.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Replace 'your_predictions_file.json' with the path to your JSON prediction file\n",
        "file_path = '/content/updated_test_dataset_with_predictions.json'\n",
        "df = pd.read_json(file_path, lines=True)  # Use lines=True if your JSON is in newline-delimited format\n",
        "\n",
        "# View the first few rows of the DataFrame\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "KTswutHv8jVG",
        "outputId": "468513c5-de1d-47b7-df44-eb99f79cc9e3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  impact_level                                       news_content  \\\n",
            "0         high  Des protestataires ont occupé temporairement d...   \n",
            "1         high  Les protestataires ont désigné le ministère co...   \n",
            "2         high  Sous les directives de l'ancien général Sami R...   \n",
            "3         high  Lors d'opérations apparemment coordonnées, des...   \n",
            "4         high  Le ministère des Affaires étrangères a été pri...   \n",
            "\n",
            "   impact_duration  predicted_impact_duration  \n",
            "0                0                          0  \n",
            "1                0                          0  \n",
            "2                0                          0  \n",
            "3                0                          0  \n",
            "4                0                          0  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Replace 'your_predictions_file.json' with the path to your JSON prediction file\n",
        "file_path = '/content/updated_test_dataset_with_predictions.json'\n",
        "df = pd.read_json(file_path, lines=True)  # Use lines=True if your JSON is in newline-delimited format\n",
        "\n",
        "# View the first few rows of the DataFrame\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "xQ7umY2gJPG9",
        "outputId": "2499edd5-6d6d-4bad-a188-5cb3fd01c830",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  impact_level                                       news_content  \\\n",
            "0         high  Des protestataires ont occupé temporairement d...   \n",
            "1         high  Les protestataires ont désigné le ministère co...   \n",
            "2         high  Sous les directives de l'ancien général Sami R...   \n",
            "3         high  Lors d'opérations apparemment coordonnées, des...   \n",
            "4         high  Le ministère des Affaires étrangères a été pri...   \n",
            "\n",
            "   impact_duration  predicted_impact_duration  \n",
            "0                0                          0  \n",
            "1                0                          0  \n",
            "2                0                          0  \n",
            "3                0                          0  \n",
            "4                0                          0  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Assuming df is your DataFrame after loading the dataset\n",
        "# Replace '/path/to/your/dataset.json' with the path to your dataset\n",
        "\n",
        "file_path = '/content/FR.xlsx'\n",
        "df = pd.read_excel(file_path)\n",
        "\n",
        "\n",
        "# Encode the 'impact_duration' labels to numeric values\n",
        "label_encoder = LabelEncoder()\n",
        "df['impact_level_encoded'] = label_encoder.fit_transform(df['impact_level'])\n",
        "# Check for NaN values in 'news_content'\n",
        "if df['news_content'].isnull().any():\n",
        "    # Handle NaN values, e.g., by replacing them with a placeholder string\n",
        "    df['news_content'].fillna('No content', inplace=True)\n",
        "\n",
        "# Split the data into training and validation sets (90% training, 10% validation)\n",
        "train_df, val_df = train_test_split(df, test_size=0.1, random_state=42)\n",
        "\n",
        "# Initialize the tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
        "\n",
        "def tokenize_data(df):\n",
        "    # Ensure all entries are strings and handle missing values if necessary\n",
        "    texts = df['news_content'].astype(str).tolist()  # Convert to string to avoid issues\n",
        "    return tokenizer(texts, truncation=True, padding=True, max_length=512)\n",
        "\n",
        "\n",
        "train_encodings = tokenize_data(train_df)\n",
        "val_encodings = tokenize_data(val_df)\n",
        "\n",
        "train_labels = train_df['impact_level_encoded'].tolist()\n",
        "val_labels = val_df['impact_level_encoded'].tolist()\n",
        "\n",
        "# Create a custom dataset for PyTorch\n",
        "class NewsDataset(Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "train_dataset = NewsDataset(train_encodings, train_labels)\n",
        "val_dataset = NewsDataset(val_encodings, val_labels)\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-multilingual-cased', num_labels=len(label_encoder.classes_))\n",
        "\n",
        "\n",
        "import os\n",
        "from transformers import TrainingArguments, Trainer\n",
        "import numpy as np\n",
        "\n",
        "# Specify the directory to save checkpoints\n",
        "checkpoint_dir = '/content/drive/MyDrive/CHECKPOINTDIR_ROBERTABASE_IMPACT_LEVEL'\n",
        "# Specify the directory to save predicted results\n",
        "predicted_results_dir = '/content/drive/MyDrive/PREDICTED RESULTS_ROBERTA_BASE_IMPACT_LEVEL'\n",
        "\n",
        "# Create the directories if they do not exist\n",
        "os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "os.makedirs(predicted_results_dir, exist_ok=True)\n",
        "\n",
        "# Define training arguments, including the checkpoint directory\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=checkpoint_dir,  # Checkpoints directory\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=16,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_steps=500,  # Save a checkpoint every 500 steps\n",
        "    save_total_limit=3  # Keep only the last 3 checkpoints\n",
        ")\n",
        "\n",
        "# Initialize the Trainer with the training arguments\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "7b88J1YwJS6K",
        "outputId": "1cdf0441-1ece-4054-ad53-1ce00ec59a3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 213
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1137' max='1137' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1137/1137 04:07, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.441400</td>\n",
              "      <td>0.245676</td>\n",
              "      <td>0.923838</td>\n",
              "      <td>0.921833</td>\n",
              "      <td>0.925363</td>\n",
              "      <td>0.923838</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Checkpoint destination directory /content/drive/MyDrive/CHECKPOINTDIR_ROBERTABASE_IMPACT_LEVEL/checkpoint-500 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=1137, training_loss=0.5205163074986602, metrics={'train_runtime': 247.4284, 'train_samples_per_second': 36.75, 'train_steps_per_second': 4.595, 'total_flos': 1742966571602466.0, 'train_loss': 0.5205163074986602, 'epoch': 1.0})"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls \"/content/drive/My Drive/CHECKPOINTDIR_ROBERTABASE_IMPACT_LEVEL\""
      ],
      "metadata": {
        "id": "djC5bCi5JUFR",
        "outputId": "a0b546a9-4347-4cd3-fd29-8ce34cc4f375",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ls: cannot access '/content/drive/My Drive/CHECKPOINTDIR_ROBERTABASE_IMPACT_LEVEL': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the best model from checkpoint for evaluation\n",
        "best_model_path = '/content/drive/My Drive/CHECKPOINTDIR_ROBERTABASE_IMPACT_LEVEL/checkpoint-1000'\n",
        "model = BertForSequenceClassification.from_pretrained(best_model_path, num_labels=len(label_encoder.classes_))\n",
        "\n",
        "\n",
        "# Evaluate the model on training and validation datasets\n",
        "train_results = trainer.evaluate(train_dataset)\n",
        "val_results = trainer.evaluate(val_dataset)\n",
        "\n",
        "# Print the evaluation results\n",
        "print(\"Training dataset performance:\", train_results)\n",
        "print(\"Validation dataset performance:\", val_results)"
      ],
      "metadata": {
        "id": "rY545TffJcR-",
        "outputId": "fd76fedc-92ce-4529-fb45-bcbff2ec0a12",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 529
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "Incorrect path_or_model_id: '/content/drive/My Drive/CHECKPOINTDIR_ROBERTABASE_IMPACT_LEVEL/checkpoint-1000'. Please provide either the path to a local folder or the repo_id of a model on the Hub.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHFValidationError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0;31m# Load from URL or cache if already cached\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m         resolved_file = hf_hub_download(\n\u001b[0m\u001b[1;32m    386\u001b[0m             \u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    109\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0marg_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"repo_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"from_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"to_id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m                 \u001b[0mvalidate_repo_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36mvalidate_repo_id\u001b[0;34m(repo_id)\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrepo_id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m         raise HFValidationError(\n\u001b[0m\u001b[1;32m    159\u001b[0m             \u001b[0;34m\"Repo id must be in the form 'repo_name' or 'namespace/repo_name':\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHFValidationError\u001b[0m: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/content/drive/My Drive/CHECKPOINTDIR_ROBERTABASE_IMPACT_LEVEL/checkpoint-1000'. Use `repo_type` argument if needed.",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-bec30cd0f788>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load the best model from checkpoint for evaluation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mbest_model_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/My Drive/CHECKPOINTDIR_ROBERTABASE_IMPACT_LEVEL/checkpoint-1000'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertForSequenceClassification\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_model_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2924\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPretrainedConfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2925\u001b[0m                 \u001b[0;31m# We make a call to the config file first (which may be absent) to get the commit hash as soon as possible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2926\u001b[0;31m                 resolved_config_file = cached_file(\n\u001b[0m\u001b[1;32m   2927\u001b[0m                     \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2928\u001b[0m                     \u001b[0mCONFIG_NAME\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    448\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mEnvironmentError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"There was a specific connection error when trying to load {path_or_repo_id}:\\n{err}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mHFValidationError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 450\u001b[0;31m         raise EnvironmentError(\n\u001b[0m\u001b[1;32m    451\u001b[0m             \u001b[0;34mf\"Incorrect path_or_model_id: '{path_or_repo_id}'. Please provide either the path to a local folder or the repo_id of a model on the Hub.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m         ) from e\n",
            "\u001b[0;31mOSError\u001b[0m: Incorrect path_or_model_id: '/content/drive/My Drive/CHECKPOINTDIR_ROBERTABASE_IMPACT_LEVEL/checkpoint-1000'. Please provide either the path to a local folder or the repo_id of a model on the Hub."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Assuming the label encoder has been fitted with your class names\n",
        "target_names = label_encoder.classes_\n",
        "\n",
        "# Training Dataset Metrics\n",
        "train_accuracy = accuracy_score(train_true_labels, train_pred_labels)\n",
        "print(\"Training Dataset Metrics:\")\n",
        "print(f\"Accuracy: {train_accuracy}\")\n",
        "print(classification_report(train_true_labels, train_pred_labels, target_names=target_names))\n",
        "\n",
        "# Validation Dataset Metrics\n",
        "val_accuracy = accuracy_score(val_true_labels, val_pred_labels)\n",
        "print(\"Validation Dataset Metrics:\")\n",
        "print(f\"Accuracy: {val_accuracy}\")\n",
        "print(classification_report(val_true_labels, val_pred_labels, target_names=target_names))\n"
      ],
      "metadata": {
        "id": "EVARza6mJeHQ",
        "outputId": "951ed3cd-6c4a-4382-fc13-bf3ba65f55cf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Dataset Metrics:\n",
            "Accuracy: 0.9861431870669746\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        high       0.99      0.99      0.99      2862\n",
            "         low       0.99      0.98      0.98      3037\n",
            "      medium       0.99      0.99      0.99      3194\n",
            "\n",
            "    accuracy                           0.99      9093\n",
            "   macro avg       0.99      0.99      0.99      9093\n",
            "weighted avg       0.99      0.99      0.99      9093\n",
            "\n",
            "Validation Dataset Metrics:\n",
            "Accuracy: 0.9525222551928784\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        high       0.96      0.95      0.95       330\n",
            "         low       0.94      0.95      0.94       311\n",
            "      medium       0.96      0.96      0.96       370\n",
            "\n",
            "    accuracy                           0.95      1011\n",
            "   macro avg       0.95      0.95      0.95      1011\n",
            "weighted avg       0.95      0.95      0.95      1011\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the test dataset\n",
        "test_dataset_path = '/content/ML-ESG3_Testset_French.json'\n",
        "test_df = pd.read_json(test_dataset_path)\n",
        "\n",
        "# Display the first few rows of the test dataset to understand its structure\n",
        "test_df.head()\n",
        "\n",
        "from transformers import BertTokenizer\n",
        "from torch.utils.data import Dataset\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Assuming you have already loaded your model and tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
        "\n",
        "# Tokenize the test dataset\n",
        "def tokenize_data(df):\n",
        "    texts = df['news_content'].astype(str).tolist()  # Ensure all entries are strings\n",
        "    return tokenizer(texts, truncation=True, padding=True, max_length=512)\n",
        "\n",
        "test_encodings = tokenize_data(test_df)\n",
        "\n",
        "# Dummy labels for creating the Dataset object, since labels are not needed for prediction\n",
        "test_labels = [0] * len(test_df)\n",
        "\n",
        "class TestDataset(Dataset):\n",
        "    def __init__(self, encodings):\n",
        "        self.encodings = encodings\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encodings.input_ids)\n",
        "\n",
        "test_dataset = TestDataset(test_encodings)\n",
        "\n",
        "# Predict using the trained model\n",
        "predictions = trainer.predict(test_dataset)\n",
        "\n",
        "# Convert predictions to labels\n",
        "predicted_labels = np.argmax(predictions.predictions, axis=1)\n",
        "original_labels = label_encoder.inverse_transform(predicted_labels)\n",
        "\n",
        "# Add predictions to the test DataFrame\n",
        "test_df['predicted_impact_level'] = original_labels\n",
        "import os\n",
        "print(os.getcwd())\n",
        "\n",
        "# Save the updated DataFrame to a new JSON file in the current working directory\n",
        "output_filename = 'updated_test_dataset_with_predictions_impact_level.json'\n",
        "test_df.to_json(output_filename, orient='records', lines=True)\n",
        "\n",
        "print(f\"Updated test dataset saved to {output_filename}\")\n"
      ],
      "metadata": {
        "id": "JTBHNJW3Jfse",
        "outputId": "8cb242fa-521c-4a0d-f7ca-25bc425ef9d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Updated test dataset saved to updated_test_dataset_with_predictions_impact_level.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Replace 'your_predictions_file.json' with the path to your JSON prediction file\n",
        "file_path = 'updated_test_dataset_with_predictions_impact_level.json'\n",
        "df = pd.read_json(file_path, lines=True)  # Use lines=True if your JSON is in newline-delimited format\n",
        "\n",
        "# View the first few rows of the DataFrame\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "xRpKZi2JJhxw",
        "outputId": "971452dc-8166-45a9-c71e-3ad91bb82299",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                 URL  \\\n",
            "0  https://www.novethic.fr/actualite/energie/tran...   \n",
            "1  https://www.novethic.fr/actualite/energie/ener...   \n",
            "2  https://www.novethic.fr/actualite/energie/ener...   \n",
            "3  https://www.novethic.fr/actualite/energie/mobi...   \n",
            "4  https://www.novethic.fr/actualite/energie/mobi...   \n",
            "\n",
            "                                          news_title  \\\n",
            "0  La France porte un projet de pipeline qui tran...   \n",
            "1  Accélération des énergies renouvelables : les ...   \n",
            "2  Accélération des énergies renouvelables : les ...   \n",
            "3  L’industrie automobile en route vers l’électri...   \n",
            "4  Le secteur ferroviaire s'organise pour rendre ...   \n",
            "\n",
            "                                        news_content predicted_impact_level  \n",
            "0  L’industrie constitue le fer de lance de la ma...                 medium  \n",
            "1  L’examen du projet de loi sur l’accélération d...                   high  \n",
            "2  3/ Le dernier mot aux maires C’était un autre ...                 medium  \n",
            "3  L’industrie automobile apparaît bien engagée p...                    low  \n",
            "4  Le train est la star de la mobilité écologique...                 medium  \n"
          ]
        }
      ]
    }
  ]
}